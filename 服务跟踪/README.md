
* [服务追踪](#服务追踪)
* [服务追踪的作⽤](#服务追踪的作⽤)
* [服务追踪系统架构](#服务追踪系统架构)
  * [数据采集层---负责数据埋点并上报](#数据采集层---负责数据埋点并上报)
  * [数据处理层---负责数据的存储与计算](#数据处理层---负责数据的存储与计算)
    * [实时数据处理]()
      * [Storm]()
      * [Spark Streaming]()
      * [HBase]()
    * [离线数据处理]()
      * [MapReduce]()
      * [Spark]()
      * [Hive]()
  * [数据展示层---负责数据的图形化展示](#数据展示层---负责数据的图形化展示)
    * [调⽤链路图---主要做故障定位]()
    * [调⽤拓扑图---主要⽤作全局监控，⽤于发现系统中异常的点]()
* [服务追踪系统选型](#服务追踪系统选型)
  * [Zipkin---Twitter](#Zipkin)
  * [Pinpoint](#Pinpoint)
  * [阿里的鹰眼](#阿里的鹰眼)
  * [美团的MTrace](#美团的MTrace等)

---

# 服务追踪
    如果有⼀个系统，可以跟踪记录⼀次⽤户请求都发起了哪些调⽤，经过哪些服务处理，并且记录每⼀次调⽤所涉及的服务的详细信息，这时候如果发⽣调⽤失败，你
    就可以通过这个⽇志快速定位是在哪个环节出了问题，这个系统就是今天我要讲解的服务追踪系统
    
    除了需要对服务调⽤情况进⾏监控之外，你还需要记录服务调⽤经过的每⼀层链路，以便进⾏问题追踪和故障定位。服务追踪的⼯作原理⼤致如下：
    
    服务消费者发起调⽤前，会在本地按照⼀定的规则⽣成⼀个requestid，发起调⽤时，将requestid当作请求参数的⼀部分，传递给服务提供者。
    
    服务提供者接收到请求后，记录下这次请求的requestid，然后处理请求。如果服务提供者继续请求其他服务，会在本地再⽣成⼀个⾃⼰的requestid，然后把这
    两个requestid都当作请求参数继续往下传递。
    
    以此类推，通过这种层层往下传递的⽅式，⼀次请求，⽆论最后依赖多少次服务调⽤、经过多少服务节点，都可以通过最开始⽣成的requestid串联所有节点，从
    ⽽达到服务追踪的⽬的。

# 服务追踪的作⽤

    第⼀，优化系统瓶颈。
    
    通过记录调⽤经过的每⼀条链路上的耗时，我们能快速定位整个系统的瓶颈点在哪⾥。⽐如你访问微博⾸⻚发现很慢，肯定是由于某种原因造成的，有可能是运营
    商⽹络延迟，有可能是⽹关系统异常，有可能是某个服务异常，还有可能是缓存或者数据库异常。通过服务追踪，可以从全局视⻆上去观察，找出整个系统的瓶颈
    点所在，然后做出针对性的优化。
    
    第⼆，优化链路调⽤。
    
    通过服务追踪可以分析调⽤所经过的路径，然后评估是否合理。⽐如⼀个服务调⽤下游依赖了多个服务，通过调⽤链分析，可以评估是否每个依赖都是必要的，是
    否可以通过业务优化来减少服务依赖。
    
    第三，⽣成⽹络拓扑。
    
    通过服务追踪系统中记录的链路信息，可以⽣成⼀张系统的⽹络调⽤拓扑图，它可以反映系统都依赖了哪些服务，以及服务之间的调⽤关系是什么样的，可以⼀⽬
    了然。除此之外，在⽹络拓扑图上还可以把服务调⽤的详细信息也标出来，也能起到服务监控的作⽤。
    
    第四，透明传输数据。
    
    除了服务追踪，业务上经常有⼀种需求，期望能把⼀些⽤户数据，从调⽤的开始⼀直往下传递，以便系统中的各个服务都能获取到这个信息。⽐如业务想做⼀些A/B
    测试，这时候就想通过服务追踪系统，把A/B测试的开关逻辑⼀直往下传递，经过的每⼀层服务都能获取到这个开关值，就能够统⼀进⾏A/B测试。
    
# 服务追踪系统架构

  <a href="https://ibb.co/d41xt1Z"><img src="https://i.ibb.co/zVjDFjd/image.jpg" alt="image" border="0"></a>
  
  
##  服务追踪系统可以分为三层。
    
  ### 数据采集层---负责数据埋点并上报
  
     数据采集层的作⽤就是在系统的各个不同模块中进⾏埋点，采集数据并上报给数据处理层进⾏处理。
  
  
  ### 数据处理层---负责数据的存储与计算
  
    数据处理层的作⽤就是把数据采集层上报的数据按需计算，然后落地存储供查询使⽤。据我所知，数据处理的需求⼀般分为两类，⼀类是实时计算需求，⼀类是
    离线计算需求。实时计算需求对计算效率要求⽐较⾼，⼀般要求对收集的链路数据能够在秒级别完成聚合计算，以供实时查询。⽽离线计算需求对计算效率要求就
    没那么⾼了，⼀般能在⼩时级别完成链路数据的聚合计算即可，⼀般⽤作数据汇总统计。
    
    针对这两类不同的数据处理需求，采⽤的计算⽅法和存储也不相同。
    
    * 实时数据处理
    
    针对实时数据处理，⼀般采⽤Storm或者Spark Streaming来对链路数据进⾏实时聚合加⼯，存储⼀般使⽤OLTP数据仓库，⽐如HBase，使⽤traceId作为
    RowKey，能天然地把⼀整条调⽤链聚合在⼀起，提⾼查询效率。
    
    * 离线数据处理
    
    针对离线数据处理，⼀般通过运⾏MapReduce或者Spark批处理程序来对链路数据进⾏离线计算，存储⼀般使⽤Hive
  
  
  ### 数据展示层---负责数据的图形化展示
  
  数据展示层的作⽤就是将处理后的链路信息以图形化的⽅式展示给⽤户。
  
  根据我的经验，实际项⽬中主要⽤到两种图形展示，⼀种是调⽤链路图，⼀种是调⽤拓扑图。
  
     * 调⽤链路图
       
       调⽤链路图在实际项⽬中，主要是被⽤来做故障定位，⽐如某⼀次⽤户调⽤失败了，可以通过调⽤链路图查询这次⽤户调⽤经过了哪些环节，到底是哪⼀层的
       调⽤失败所导致
  
     * 调⽤拓扑图
  
       调⽤拓扑图是⼀种全局视野图，在实际项⽬中，主要⽤作全局监控，⽤于发现系统中异常的点，从⽽快速做出决策。⽐如，某⼀个服务突然出现异常，那么在调
       ⽤链路拓扑图中可以看出对这个服务的调⽤耗时都变⾼了，可以⽤红⾊的图样标出来，⽤作监控报警。
       
       
# 服务追踪系统选型

### Zipkin
     
     OpenZipkin收集到的数据只到接⼝级别，进⼀步的信息就没有了。
     
     同理在绘制链路拓扑图时，OpenZipkin只能绘制服务与服务之间的调⽤链路拓扑图
     
### Pinpoint

    因为Pinpoint采⽤了字节码注⼊的⽅式实现trace信息收集，所以它能拿到的信息⽐OpenZipkin多得多。从下⾯这张图可以看出，它不仅能够查看接⼝级别的链
    路调⽤信息，还能深⼊到调⽤所关联的数据库信息
    
    ⽽Pinpoint不仅能够绘制服务与服务之间，还能绘制与DB之间的调⽤链路拓扑图
